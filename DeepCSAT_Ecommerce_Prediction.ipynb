{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcdc64ee",
   "metadata": {},
   "source": [
    "# DeepCSAT_Ecommerce_Prediction\n",
    "\n",
    "**Project:** E-commerce Customer Satisfaction Prediction (Deep Learning ANN)\n",
    "\n",
    "**Author:** Mahima Patel\n",
    "\n",
    "**Description:** This single notebook contains the full pipeline: data loading, exploratory data analysis (15 charts with explanations), data cleaning, feature engineering, ANN model building (Keras), training, evaluation, saving model, and conclusions. Each code cell is followed by a clear markdown explanation so it is ready to submit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0214b40a-0ef2-4239-afd6-fe804667d5f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['C:\\\\Users\\\\hp\\\\anaconda3\\\\python.exe', '-m', 'pip', 'install', 'tensorflow']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m, in \u001b[0;36minstall_if_missing\u001b[1;34m(package)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(package)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1324\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m         subprocess\u001b[38;5;241m.\u001b[39mcheck_call([sys\u001b[38;5;241m.\u001b[39mexecutable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall\u001b[39m\u001b[38;5;124m\"\u001b[39m, package],\n\u001b[0;32m     10\u001b[0m                               stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pkg \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseaborn\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 13\u001b[0m     install_if_missing(pkg)\n",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m, in \u001b[0;36minstall_if_missing\u001b[1;34m(package)\u001b[0m\n\u001b[0;32m      7\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mimport_module(package)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mcheck_call([sys\u001b[38;5;241m.\u001b[39mexecutable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall\u001b[39m\u001b[38;5;124m\"\u001b[39m, package],\n\u001b[0;32m     10\u001b[0m                           stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL, stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\subprocess.py:419\u001b[0m, in \u001b[0;36mcheck_call\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cmd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    418\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m popenargs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['C:\\\\Users\\\\hp\\\\anaconda3\\\\python.exe', '-m', 'pip', 'install', 'tensorflow']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package],\n",
    "                              stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "for pkg in [\"numpy\", \"pandas\", \"tensorflow\", \"keras\", \"scikit-learn\", \"matplotlib\", \"seaborn\"]:\n",
    "    install_if_missing(pkg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9384c6e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "The following cell imports all required libraries. Explanations for each import are given inline in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical and data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting (matplotlib only to keep plots reproducible in many environments)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Deep learning (Keras API)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# File system\n",
    "import os\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455def7",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load dataset from the `data/` folder. The code below reads the CSV and shows a quick preview and basic info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc39d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust path if needed. The uploaded file is expected at: data/eCommerce_Customer_support_data.csv\n",
    "data_path = 'data/eCommerce_Customer_support_data.csv'\n",
    "if not os.path.exists(data_path):\n",
    "    # fallback to root if user placed file differently in the environment used here\n",
    "    data_path = '/mnt/data/eCommerce_Customer_support_data.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print('Dataset shape:', df.shape)\n",
    "display(df.head(5))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455531da",
   "metadata": {},
   "source": [
    "### 2.1 Initial EDA: Basic statistics and missing values\n",
    "\n",
    "We check summary statistics and missing values to plan cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7245588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "display(df.describe(include='all').T)\n",
    "\n",
    "# Missing values\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing = missing[missing>0]\n",
    "print('Columns with missing values:')\n",
    "display(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e061d",
   "metadata": {},
   "source": [
    "### 2.2 Identify numeric and categorical columns\n",
    "\n",
    "We will programmatically detect numeric and categorical columns so the plotting and preprocessing adapts to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print('Numeric columns:', numeric_cols)\n",
    "print('Categorical columns:', cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12d8ee5",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (15 charts)\n",
    "\n",
    "Below are 15 charts. Each chart cell includes a short markdown explaining what to look for, why we chose the chart, and how the insight helps model training.\n",
    "\n",
    "### Chart 1-6: Histograms for numeric features (distribution insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081e7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charts 1-6: Histograms for numeric features\n",
    "num_to_plot = numeric_cols[:6]  # up to 6 histograms\n",
    "for col in num_to_plot:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(df[col].dropna(), bins=30)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf55e2e",
   "metadata": {},
   "source": [
    "**Insight & Reason:**\n",
    "\n",
    "- **What to look for:** skewness, multimodality, outliers.\n",
    "- **Why chosen:** Histograms show how feature values are distributed. If strongly skewed, consider transforms (log) which can help ANN training by stabilizing gradients.\n",
    "- **How it helps model training:** Normalized distributions or transformed features usually make neural network training smoother and improve convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62970faf",
   "metadata": {},
   "source": [
    "### Chart 7-10: Bar plots for top categorical features (category counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0083dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charts 7-10: Bar plots for top categorical features\n",
    "cat_to_plot = cat_cols[:4]  # up to 4 categorical plots\n",
    "for col in cat_to_plot:\n",
    "    vc = df[col].value_counts().nlargest(10)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    vc.plot(kind='bar')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Top categories in {col}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc187886",
   "metadata": {},
   "source": [
    "**Insight & Reason:**\n",
    "\n",
    "- **What to look for:** dominant categories, class imbalance.\n",
    "- **Why chosen:** Bar plots quickly reveal category frequency; imbalance may require special handling (class weights or resampling) for the ANN.\n",
    "- **How it helps model training:** Knowing imbalance helps choose loss/metrics or resampling strategies to avoid biased models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5b0e6",
   "metadata": {},
   "source": [
    "### Chart 11: Correlation heatmap (numeric features)\n",
    "\n",
    "Shows pairwise correlations, helps detect multicollinearity and features strongly correlated with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351be12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 11: Correlation matrix (numeric)\n",
    "if len(numeric_cols) > 1:\n",
    "    corr = df[numeric_cols].corr()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    im = plt.imshow(corr, interpolation='nearest')\n",
    "    plt.colorbar(im)\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.index)), corr.index)\n",
    "    plt.title('Correlation matrix (numeric features)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Not enough numeric columns for correlation matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e2756",
   "metadata": {},
   "source": [
    "**Insight & Reason:**\n",
    "\n",
    "- **What to look for:** features highly correlated with target or with each other.\n",
    "- **Why chosen:** Correlation can suggest which features matter and whether multicollinearity might harm model learning.\n",
    "- **How it helps model training:** If two features are nearly identical, consider dropping one to reduce redundancy and overfitting risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d09eb",
   "metadata": {},
   "source": [
    "### Chart 12: Boxplot of a numeric feature vs target\n",
    "\n",
    "This visualizes how a numeric feature's distribution varies across target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaad3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 12: Boxplot for numeric vs target (if target exists)\n",
    "target_col = None\n",
    "# heuristics to find a likely target column named like 'CSAT' or 'target'\n",
    "for possible in ['CSAT_Score', 'CSAT', 'csat', 'target', 'label']:\n",
    "    if possible in df.columns:\n",
    "        target_col = possible\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    # fallback: if dataset has a low-cardinality numeric column, treat it as target\n",
    "    numeric_card = [(c, df[c].nunique()) for c in numeric_cols]\n",
    "    numeric_card.sort(key=lambda x: x[1])\n",
    "    if numeric_card and numeric_card[0][1] <= 10:\n",
    "        target_col = numeric_card[0][0]\n",
    "\n",
    "if target_col is None:\n",
    "    print('No obvious target column found for boxplot. Please set target_col variable manually.')\n",
    "else:\n",
    "    # choose a numeric feature different from target\n",
    "    num_feats = [c for c in numeric_cols if c != target_col]\n",
    "    if num_feats:\n",
    "        col = num_feats[0]\n",
    "        # draw boxplots per target class\n",
    "        classes = sorted(df[target_col].dropna().unique())\n",
    "        plt.figure(figsize=(6,4))\n",
    "        data_to_plot = [df[df[target_col]==cls][col].dropna() for cls in classes]\n",
    "        plt.boxplot(data_to_plot, labels=[str(x) for x in classes])\n",
    "        plt.xlabel('Target ('+target_col+')')\n",
    "        plt.ylabel(col)\n",
    "        plt.title(f'Boxplot of {col} grouped by {target_col}')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No numeric features available for boxplot.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82b90f",
   "metadata": {},
   "source": [
    "**Insight & Reason:**\n",
    "\n",
    "- **What to look for:** differences in medians, spread and outliers across target classes.\n",
    "- **Why chosen:** Boxplots reveal whether a numeric feature separates target classes well (useful feature for classification).\n",
    "- **How it helps model training:** Good separability indicates a strong predictive feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c356d8",
   "metadata": {},
   "source": [
    "### Chart 13-14: Scatter plots between numeric feature pairs (relationship / interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 13-14: Scatter plots for top numeric pairs\n",
    "num_pairs = []\n",
    "if len(numeric_cols) >= 2:\n",
    "    num_pairs = [(numeric_cols[i], numeric_cols[i+1]) for i in range(min(2, len(numeric_cols)-1))]\n",
    "for a,b in num_pairs:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(df[a], df[b], alpha=0.5, s=10)\n",
    "    plt.xlabel(a)\n",
    "    plt.ylabel(b)\n",
    "    plt.title(f'Scatter: {a} vs {b}')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd98045",
   "metadata": {},
   "source": [
    "**Insight & Reason:**\n",
    "\n",
    "- **What to look for:** linear or non-linear relationships and clusters.\n",
    "- **Why chosen:** Scatter plots reveal interactions that a neural network can exploit.\n",
    "- **How it helps model training:** If two features interact strongly, the ANN can model complex combinations of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f008d3",
   "metadata": {},
   "source": [
    "### Chart 15: Target distribution\n",
    "\n",
    "Visualize the target classes to understand balance/imbalance prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 15: Target distribution (if target exists)\n",
    "if target_col is None:\n",
    "    print('No target column detected automatically. Set target_col manually to visualize target distribution.')\n",
    "else:\n",
    "    vals = df[target_col].value_counts().sort_index()\n",
    "    plt.figure(figsize=(6,4))\n",
    "    vals.plot(kind='bar')\n",
    "    plt.xlabel(target_col)\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Distribution of target: {target_col}')\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcc00c",
   "metadata": {},
   "source": [
    "**Insight & Reason:**\n",
    "\n",
    "- **What to look for:** whether the classes are balanced.\n",
    "- **Why chosen:** ANN training on imbalanced data can produce biased predictions; we may need class weights or resampling.\n",
    "- **How it helps model training:** Guides selection of loss function, metrics, and sampling strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadfe264",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning & Preprocessing\n",
    "\n",
    "This section performs cleaning, missing value handling, encoding and scaling. Code is adaptive to dataset columns detected earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframe to work on\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Identify or set target column if not found earlier\n",
    "if 'target_col' in globals() and target_col is not None:\n",
    "    target = target_col\n",
    "else:\n",
    "    # try common names\n",
    "    for possible in ['CSAT_Score', 'CSAT', 'csat', 'target', 'label']:\n",
    "        if possible in df_clean.columns:\n",
    "            target = possible\n",
    "            break\n",
    "    else:\n",
    "        # fallback: ask user to set target manually\n",
    "        target = None\n",
    "\n",
    "print('Auto-detected target column:', target)\n",
    "\n",
    "# Drop obvious ID columns\n",
    "for idcol in ['Ticket_ID', 'Customer_ID', 'ID', 'id']:\n",
    "    if idcol in df_clean.columns:\n",
    "        df_clean.drop(columns=[idcol], inplace=True, errors='ignore')\n",
    "\n",
    "# Fill numeric missing with mean, categorical with mode\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].dtype in [np.float64, np.int64]:\n",
    "        df_clean[col].fillna(df_clean[col].mean(), inplace=True)\n",
    "    else:\n",
    "        df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "\n",
    "# Encode categorical features\n",
    "le_map = {}\n",
    "for col in df_clean.select_dtypes(include=['object', 'category']).columns:\n",
    "    le = LabelEncoder()\n",
    "    df_clean[col] = le.fit_transform(df_clean[col].astype(str))\n",
    "    le_map[col] = le\n",
    "\n",
    "print('Preprocessing complete. Cleaned shape:', df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581b6a1f",
   "metadata": {},
   "source": [
    "### Feature matrix and target vector\n",
    "\n",
    "We prepare `X` and `y`, then scale features using `StandardScaler` for ANN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d272b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure target is set\n",
    "if target is None:\n",
    "    raise ValueError('Please set the target variable name in the notebook (target variable could not be auto-detected).')\n",
    "\n",
    "X = df_clean.drop(columns=[target])\n",
    "y = df_clean[target]\n",
    "\n",
    "# If target is numeric with many unique values, consider converting to binary or classes\n",
    "if y.nunique() > 5:\n",
    "    print('Detected many unique values in target; assuming classification may be binary/multiclass. Keep as-is.')\n",
    "else:\n",
    "    print('Target classes:', y.unique())\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y if y.nunique()>1 else None)\n",
    "\n",
    "print('Train/Test shapes:', X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f91963",
   "metadata": {},
   "source": [
    "## 5. Build and Train ANN (Deep Learning)\n",
    "\n",
    "We build a Multilayer Perceptron using Keras. This architecture is a strong baseline for tabular data and meets the 'DeepCSAT' deep learning requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a827c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine output layer configuration\n",
    "num_classes = y_train.nunique() if hasattr(y_train, 'nunique') else len(np.unique(y_train))\n",
    "print('Detected number of target classes:', num_classes)\n",
    "\n",
    "# If binary classification\n",
    "if num_classes == 2:\n",
    "    output_units = 1\n",
    "    output_activation = 'sigmoid'\n",
    "    loss_fn = 'binary_crossentropy'\n",
    "else:\n",
    "    # multiclass\n",
    "    output_units = num_classes\n",
    "    output_activation = 'softmax'\n",
    "    loss_fn = 'sparse_categorical_crossentropy'\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(output_units, activation=output_activation))\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5b3d4",
   "metadata": {},
   "source": [
    "### Train the model with EarlyStopping\n",
    "\n",
    "We use early stopping to avoid overfitting and restore the best weights observed on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d428be",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49029371",
   "metadata": {},
   "source": [
    "### Training Curves: Accuracy and Loss\n",
    "\n",
    "Plot training and validation accuracy and loss to inspect learning behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77cc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss curves\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history.get('accuracy', []), label='train_accuracy')\n",
    "plt.plot(history.history.get('val_accuracy', []), label='val_accuracy')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history.get('loss', []), label='train_loss')\n",
    "plt.plot(history.history.get('val_loss', []), label='val_loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f1fd5",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation on Test Set\n",
    "\n",
    "We compute predictions and standard classification metrics, plus a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef80dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "if num_classes == 2:\n",
    "    y_prob = model.predict(X_test)\n",
    "    y_pred = (y_prob > 0.5).astype(int).reshape(-1)\n",
    "else:\n",
    "    y_prob = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('\\nClassification Report:\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(range(len(np.unique(y_test))))\n",
    "plt.yticks(range(len(np.unique(y_test))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59602440",
   "metadata": {},
   "source": [
    "## 7. Save Model and Predictions\n",
    "\n",
    "Save the trained model and create a submission CSV containing actual vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure models and submission directories exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('submission', exist_ok=True)\n",
    "\n",
    "model_path = 'models/best_model.h5'\n",
    "model.save(model_path)\n",
    "print('Model saved to', model_path)\n",
    "\n",
    "# Save predictions\n",
    "import pandas as pd\n",
    "pred_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "pred_df.to_csv('submission/CSAT_Predictions.csv', index=False)\n",
    "print('Predictions saved to submission/CSAT_Predictions.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88af9b4e-cd11-4619-a504-9bd2265d0e71",
   "metadata": {},
   "source": [
    "## 8. Conclusions & Next Steps\n",
    "\n",
    "**Summary of results:**\n",
    "\n",
    "- The notebook trained an ANN (MLP) on the provided structured dataset and evaluated it on a held-out test set. Metrics and confusion matrix are produced in the evaluation section.\n",
    "\n",
    "**How the 15 charts helped:**\n",
    "\n",
    "- Histograms revealed distributions and possible skew—guiding whether to transform features.\n",
    "- Bar plots showed categorical imbalances—guiding sampling/weighting decisions.\n",
    "- Correlation matrix pointed to strongly related features—helpful for feature selection or to avoid multicollinearity.\n",
    "- Boxplots and scatter plots exposed relationships between features and the target—highlighting features an ANN should learn.\n",
    "\n",
    "**Next steps / improvements:**\n",
    "\n",
    "1. Hyperparameter tuning (learning rate, architecture, batch size) using Keras Tuner or manual grid search.\n",
    "2. If target is imbalanced, try class weights, SMOTE, or focal loss.\n",
    "3. Feature importance analysis using permutation importance or SHAP for interpretability.\n",
    "4. Build a small Streamlit app to demo predictions interactively.\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck, Mahima — this notebook is ready to be run in Jupyter.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
